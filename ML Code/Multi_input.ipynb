{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "exceptional-certification",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os.path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from random import randrange\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "wound-lafayette",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_image(fig_num, img, title):\n",
    "    plt.figure(fig_num)\n",
    "    plt.imshow(img, cmap='jet')\n",
    "    plt.colorbar()\n",
    "    plt.title(title)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "interim-ottawa",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"C:/Users/UKGC/Desktop/FYP/Datasets/CNN_train/\"\n",
    "TEST_PATH = \"C:/Users/UKGC/Desktop/FYP/Datasets/CNN_test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "hired-tribune",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_generator(generator, subset, x1_dir, x2_dir, x3_dir, x4_dir, y1_dir):   \n",
    "    genX1 = generator.flow_from_directory(\n",
    "        x1_dir,\n",
    "        target_size=(64,64),\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode=None,\n",
    "        batch_size=20,\n",
    "        shuffle=False,\n",
    "        seed=23,\n",
    "        subset=subset\n",
    "    )\n",
    "    genX2 = generator.flow_from_directory(\n",
    "        x2_dir,\n",
    "        target_size=(64,64),\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode=None,\n",
    "        batch_size=20,\n",
    "        shuffle=False,\n",
    "        seed=23,\n",
    "        subset=subset\n",
    "    )\n",
    "    genX3 = generator.flow_from_directory(\n",
    "        x3_dir,\n",
    "        target_size=(64,64),\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode=None,\n",
    "        batch_size=20,\n",
    "        shuffle=False,\n",
    "        seed=23,\n",
    "        subset=subset\n",
    "    )\n",
    "    genX4 = generator.flow_from_directory(\n",
    "        x4_dir,\n",
    "        target_size=(64,64),\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode=None,\n",
    "        batch_size=20,\n",
    "        shuffle=False,\n",
    "        seed=23,\n",
    "        subset=subset\n",
    "    )\n",
    "    genY1 = generator.flow_from_directory(\n",
    "        y1_dir,\n",
    "        target_size=(64,64),\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode=None,\n",
    "        batch_size=20,\n",
    "        shuffle=False,\n",
    "        seed=23,\n",
    "        subset=subset\n",
    "    )\n",
    "\n",
    "    while True:\n",
    "        X1i = next(genX1)\n",
    "        X2i = next(genX2)\n",
    "        X3i = next(genX3)\n",
    "        X4i = next(genX4)\n",
    "        Y1i = next(genY1)\n",
    "        yield np.concatenate((X1i, X3i), 3), Y1i\n",
    "        #yield X1i, Y1i\n",
    "        \n",
    "        \n",
    "def single_generator(generator, x1_dir, subset):\n",
    "    genX1 = generator.flow_from_directory(\n",
    "        x1_dir,\n",
    "        target_size=(64,64),\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode=None,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        seed=23,\n",
    "        subset=subset\n",
    "    )\n",
    "\n",
    "    while True:\n",
    "        X1i = next(genX1)\n",
    "        yield X1i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "thorough-princeton",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2\n",
    ")\n",
    "test_generator = keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "target_size = (64,64)\n",
    "color_mode = \"grayscale\"\n",
    "class_mode = None\n",
    "batch_size = 64\n",
    "shuffle = True\n",
    "seed = 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "extra-projection",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = multi_generator(train_generator,\n",
    "                             \"training\",\n",
    "                             TRAIN_PATH+\"AA_LBP_image\",\n",
    "                             TRAIN_PATH+\"ToF_LBP_image\",\n",
    "                             TRAIN_PATH+\"AA_LBPTH_image\",\n",
    "                             TRAIN_PATH+\"avg_image\",\n",
    "                             TRAIN_PATH+\"True_image\")\n",
    "valid_data = multi_generator(train_generator,\n",
    "                             \"validation\",\n",
    "                             TRAIN_PATH+\"AA_LBP_image\",\n",
    "                             TRAIN_PATH+\"ToF_LBP_image\",\n",
    "                             TRAIN_PATH+\"AA_LBPTH_image\",\n",
    "                             TRAIN_PATH+\"avg_image\",\n",
    "                             TRAIN_PATH+\"True_image\")\n",
    "test_data = multi_generator(test_generator,\n",
    "                             None,\n",
    "                             TEST_PATH+\"AA_LBP_image\",\n",
    "                             TEST_PATH+\"ToF_LBP_image\",\n",
    "                             TEST_PATH+\"AA_LBPTH_image\",\n",
    "                             TEST_PATH+\"avg_image\",\n",
    "                             TEST_PATH+\"True_image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "corporate-withdrawal",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(64,64, 4)),\n",
    "        layers.Conv2D(16, 3, padding=\"same\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "\n",
    "        layers.Conv2D(16, 3, padding=\"same\", dilation_rate=1),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "        layers.Conv2D(16, 3, padding=\"same\", dilation_rate=1),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),  \n",
    "\n",
    "        layers.Conv2D(16, 3, padding=\"same\", dilation_rate=2),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "        layers.Conv2D(16, 3, padding=\"same\", dilation_rate=2),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "\n",
    "        layers.Conv2D(16, 3, padding=\"same\", dilation_rate=4),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "        layers.Conv2D(16, 3, padding=\"same\", dilation_rate=4),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "\n",
    "        layers.Conv2D(16, 3, padding=\"same\", dilation_rate=8),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "        layers.Conv2D(16, 3, padding=\"same\", dilation_rate=8),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "\n",
    "        layers.Conv2D(16, 3, padding=\"same\", dilation_rate=16),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "        layers.Conv2D(16, 3, padding=\"same\", dilation_rate=16),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "\n",
    "        layers.Conv2D(1, 3, padding=\"same\", activation=\"relu\")\n",
    "    ]\n",
    ")\n",
    "model.compile(\n",
    "    loss = keras.losses.MeanSquaredError(),\n",
    "    optimizer = keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "framed-passage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "93/93 [==============================] - ETA: 0s - loss: 0.0467 - accuracy: 0.9564INFO:tensorflow:Assets written to: C:/Users/UKGC/Desktop/FYP/Models/Temp\\assets\n",
      "93/93 [==============================] - 94s 1s/step - loss: 0.0467 - accuracy: 0.9564 - val_loss: 0.0544 - val_accuracy: 0.9432\n",
      "Epoch 2/2\n",
      "93/93 [==============================] - 86s 928ms/step - loss: 0.0127 - accuracy: 0.9846 - val_loss: 0.0554 - val_accuracy: 0.9427\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x = train_data,\n",
    "    validation_data=valid_data,\n",
    "    steps_per_epoch=6000//batch_size,\n",
    "    validation_steps=1500//batch_size,\n",
    "    epochs=2,\n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=10,\n",
    "        restore_best_weights=True,),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"C:/Users/UKGC/Desktop/FYP/Models/Temp/\",\n",
    "        save_best_only=True,\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_freq='epoch'),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "central-reconstruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"C:/Users/UKGC/Desktop/FYP/Models/ImgEnhCnn_aa_lbp2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "heated-witch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 images belonging to 1 classes.\n",
      "Found 20 images belonging to 1 classes.\n",
      "Found 20 images belonging to 1 classes.\n",
      "Found 20 images belonging to 1 classes.\n",
      "Found 20 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "pred_imgs, true_img = next(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "selective-paintball",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 64, 64, 2)\n",
      "(20, 64, 64, 1)\n",
      "(20, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "print(pred_imgs.shape)\n",
    "print(true_img.shape)\n",
    "predictions1 = model.predict(pred_imgs)\n",
    "predictions = np.squeeze(predictions1)\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "sixth-collectible",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 2s 98ms/step - loss: 0.0063 - accuracy: 0.9931\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_data, steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "accompanied-document",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = [1, 6, 2, 18, 20, 16]\n",
    "\n",
    "for i in range(1, 21):\n",
    "    current_avg_img = predictions[i-1] / np.max(predictions[i-1])\n",
    "    current_avg_img = current_avg_img * 255\n",
    "    image_mat = (current_avg_img).astype(np.uint8)\n",
    "    image = Image.fromarray(image_mat)\n",
    "    image.save(\"C:/Users/UKGC/Desktop/FYP/Datasets/CNN_test/Results_aa_lbp2/pred\"+str(i)+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "small-prescription",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAS4AAAD8CAYAAADJwUnTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAVoUlEQVR4nO3df5BdZX3H8fcnSyygEMBAZkliwZpqHUd+NIIO1iKIDdQa2pEWqRqZOFtmwMEZZyTYGWPaf9Kx42inaLqDkThS04yCZJhozESpdRQMv0SSEEkjDWu2pPG3UsBNvv3jnLX33uzee+7uveec5+7nNXPn3nPu2ed8d8N8eZ7nPD8UEZiZpWRe1QGYmXXLicvMkuPEZWbJceIys+Q4cZlZcpy4zCw5Tlxm1jeSNko6LOnxab6XpH+StF/SY5IuLFKuE5eZ9dMdwIo2318JLMtfI8CnixQ6q8QlaYWkfXm2XDObssxs8ETEN4GftLlkJfC5yNwPnCZpuFO5J8w0IElDwG3AFcAYsEvS1ojYM/3PnBxw2kxvaWYd/YyIZzWbEl4hxbMFrx2H3cBzDadGI2K0i9stBp5uOB7Lz423+6EZJy7gImB/RBwAkLSZLHtOm7iypDUyi1uaWXvd5IypPQv8TcFrPwrPRcTyWdxuqiTbcR7ibBLXVJny4taLJI3w22y1YBa3M7MyiNklhi6NAUsbjpcAhzr90Gz6uAplyogYjYjlWVY+eRa3M7MyzANOKvjqga3Ae/Kni68Hfh4RbZuJMLvEOqNMaWb1JmB+r8qSvgBcCiyUNAasnSw+IjYA24CrgP1krdTri5Q7m8S1C1gm6VzgR8C1wHWzKM/MaqCXTcWIeGeH7wO4sdtyZxxfRExIugnYDgwBGyNi90zLM7N66GWNq19mlVgjYhtZVc/MBkTJnfMzUvf4zKxkA1/jMrPBM/lUsc6cuMysiWtcZpakuieGusdnZiVzjcvMkuOnimaWHHfOm1ly3FQ0s+S4qWhmyXGNy8yS4xqXmSXHNS4zS47wU0UzS4yA+UUzw0Q/I5meE5eZNZHgBCcuM0uJBPOHqo6iPScusy6sZV3T8TrWTvld4/nUdFXjqkjNwzOzskkw/3eqjqI9Jy4za5bAQK6ah2eWjpSbh02cuMwsSTXPDDUPz8xKJ7INB2vMicvMmrmpaDZYBqYfqx0BfqpoZklJoMY1r9MFkjZKOizp8YZzZ0jaIenJ/P30/oZpZqWZTFxFXhXpmLiAO4AVLefWADsjYhmwMz82s0ExVPBVkY6JKyK+Cfyk5fRKYFP+eRNwdW/DMrPKJFDjmumtF0XEOEBEjEs6a7oLJY0AI9nRghnezsxKk0AfV9/Di4hRYBRAOjv6fT8zm6UBfqr4jKThvLY1DBzuZVBmVqEEalxFOuenshVYlX9eBdzTm3DMrHKD0Mcl6QvApcBCSWPAWmA9sEXSauAgcE0/gzSzEg3ClJ+IeOc0X13e41jMrA563FSUtAL4JFk6vD0i1rd8vwD4PPCy/M7/GBGfbVdmzVuyZla6HnbOSxoCbgOuAMaAXZK2RsSehstuBPZExJ9JOhPYJ+nOiHhhunJn2sdlZoOqt31cFwH7I+JAnog2k40DbRTAKZIEvIRs3GjbbThc4zKzZt01FRdKerDheDQfAjVpMfB0w/EYcHFLGf9M9sDvEHAK8FcRcazdTZ24zOx4xTPDkYhY3uZ7TXGudTznnwCPApcBvwfskPQfEfGL6Qp1U9HMmk0+VezNXMUxYGnD8RKymlWj64G7IrMf+CHwqnaFOnGZWbPe9nHtApZJOlfSi4BryZqFjQ6Sj1KQtAh4JXCgXaFuKppZsx4+VYyICUk3AdvJ6mgbI2K3pBvy7zcAfw/cIen7+d1viYgj7cp14jKzZj0exxUR24BtLec2NHw+BLy1mzKduMysWQJzFWsenpmVzonLzJKU+lxFM5tjXOMys+QM8EKCZjaoXOMys+Q4cZlZcgZhIUEzm2Nc4zKz5Ag4seog2nPiMrNmbiqaWXLcVDSzJNU8M9Q8PDMrnZuKZpYcNxVtrlrLukLXrWNtnyOxrnnKj5klJ4EaV8c15yUtlfQNSXsl7ZZ0c37+DEk7JD2Zv5/e/3DNrO96u+Z8XxS59QTwwYh4WNIpwEOSdgDvBXZGxHpJa4A1wC39C9XqpmhzsJGbhgkYhBpXRIxHxMP5518Ce8k2eVwJbMov2wRc3acYzaxsvduerC+6yquSzgEuAB4AFkXEOGTJTdJZ0/zMCDCSHS2YRahmVooEalyFw5P0EuBLwAci4hfSVBvUHi/fjns0K+Ps1h1szaxuBuWpoqT5ZEnrzoi4Kz/9jKThvLY1DBzuV5BWDzPp0+pUhvu8aiiBGleRp4oCPgPsjYiPN3y1FViVf14F3NP78MysdAPyVPES4N3A9yU9mp/7MLAe2CJpNdkW2tf0JUIzK1cCNa6O4UXEt8h+lalc3ttwrG560Ty09ITnKppZSmIevOCFBM0sJSGYGOrY/Z071tdYpuPEZcdx83BuC4mjJxRNDS/0NZbpOHGZ2XGODtW7k8uJy8yaBOJozVcSdOIysyaBmHDisrpzn5Y1CsQLNZ/z48RlZk1SaCoWfeZpZnPIUYYKvYqQtELSPkn787X7prrmUkmP5ouV/nunMl3jslJ5UnX99bKPS9IQcBtwBTAG7JK0NSL2NFxzGvApYEVEHJxuiaxGTlxm1iRrKvYsNVwE7I+IAwCSNpMtQrqn4ZrrgLsi4iBARHRcacaJy8yaZJ3zLyp6+UJJDzYcj+Zr8E1aDDzdcDwGXNxSxu8D8yXdB5wCfDIiPtfupk5cZtYkoJum4pGIWN7m+6kWaGhdUPQE4A/JFm04CfiOpPsj4gfTFerEZWYtetpUHAOWNhwvAQ5Ncc2RiPg18GtJ3wTOA6ZNXH6qaGZNJodD9Oip4i5gmaRzJb0IuJZsEdJG9wB/JOkESSeTNSX3tivUNS4zO06vxnFFxISkm4DtZPsCbYyI3ZJuyL/fEBF7JX0VeIxsuYnbI+LxduU6cSXOa7hbr/V6AGpEbAO2tZzb0HL8MeBjRct04jKzJoF43lN+zCwlKUz5ceJKnJuG1mtOXGaWJC9rY2ZJ6fGUn76od3RmVjo3Fc0sOdlTxcJzFSvhxGVmTVJoKnac8iPpREnflfS9fJGvdfn5MyTtkPRk/n56/8M1szL0ciHBfiiSVp8HLouIX0maD3xL0leAvwB2RsT6fFXDNcAtfYzV+qR1SIXXoJ/bUujj6ljjisyv8sP5+SvIFgPblJ/fBFzdjwDNrFw9nmTdF4Uasvnyqw8BrwBui4gHJC2KiHGAiBifbrlVSSPASHa0oBcxm1kfDcyUn4g4Cpyfrw19t6TXFL1BvhriKIB0dusCYmZWMyk0Fbt6dBARP8uXV10BPCNpOK9tDQMd14k2szTUPXEVeap4Zl7TQtJJwFuAJ8gWA1uVX7aKbDEwM0vc5C4/RV5VKVLjGgY25f1c84AtEXGvpO8AWyStBg4C1/QxTjMrSQrjuDpGFxGPARdMcf7HZIvbm9mAqXtTsd5p1cxK1+X2ZJVw4jKzJr3cybpfnLjMrMlA9HGZ2dzjPi4zS8rADUA1s8HnPi4zS072VHEA5iqa2dzhpqKZJcmJy5LTuLBg0UUFvb/j4HAfl5klx+O4zCw5nvJjZslxU9GS576ruclNRTNLiodDmFlynLjMLEnu4zKzpBxjXu2n/HTcLMPM5p5ebggraYWkfZL257veT3fd6yQdlfSOTmW6xmUDpehI/1YzmS0wU3V/UtvLPq58k53bgCuAMWCXpK0RsWeK6/4B2F6kXNe4zKxJQC+3J7sI2B8RByLiBWAzsHKK694PfImC+7O6xmVmLbqa8rNQ0oMNx6P57vWTFgNPNxyPARc33U1aDPw5cBnwuiI3deIysyZdNhWPRMTyNt9ryls0+wRwS0Qclaa6/HhOXFYb/e5bqsu9W+9Vtz6vQDzfu7mKY8DShuMlwKGWa5YDm/OktRC4StJERHx5ukKduMysSY9Xh9gFLJN0LvAj4Frguqb7RZw7+VnSHcC97ZIWdNE5L2lI0iOS7s2Pz5C0Q9KT+fvphX8VM6u1Xg2HiIgJ4Cayp4V7gS0RsVvSDZJumGl83aTVm/Mbn5ofrwF2RsT6fGzGGuCWmQZic0OVzUErptdTfiJiG7Ct5dyGaa59b5EyC9W4JC0B/hS4veH0SmBT/nkTcHWRssys3gJx9NhQoVdVita4PgF8CDil4dyiiBgHiIhxSWdN9YOSRoCR7GjBTOM0s5LEMfH8c/We8tMxcUl6G3A4Ih6SdGm3N8jHdIxmZZ3d+hjUzGomQhydSH+S9SXA2yVdBZwInCrp88Azkobz2tYwBUe8mlmzxn6/WgyNCGqfuDr2cUXErRGxJCLOIXuU+fWIeBewFViVX7YKuKdvUZpZaSLExG+GCr2qMpvBGuuBLZJWAweBa3oTkplVSxw7Wu8hnl1FFxH3Affln38MXN77kMysUgHUvKlY77RqZuU7Jniu3qmh3tGZWTUmqg6gPScus4rV4klio2xBrlpz4jKzZk5cZpacAH5TdRDtOXGZWbMAnq86iPacuMysmZuKZpYcJy4zS44Tl5klx4nLzJLkxGVmSTkGPFd1EO05cZlZMzcVzSw5TlxmlhwnLjNLkhOXmSXFNS4zS84x4H+rDqI9Jy4zaxbA0aqDaM+Jy8yO56aimSXFfVxmlhwnLjNLjqf8mFkZ1rIOgNFeFegal5klZVCaipKeAn5J9pB0IiKWSzoD+DfgHOAp4C8j4qf9CdPMSpPAZhnzurj2zRFxfkQsz4/XADsjYhmwMz82s9RNjuMq8qpIN4mr1UpgU/55E3D1rKMxs+pNNhWLvAqQtELSPkn7JR1XwZH015Iey1/flnRepzKL9nEF8DVJAfxLRIwCiyJiHCAixiWdNU3QI8BIdrSg4O3MrDJBz6b8SBoCbgOuAMaAXZK2RsSehst+CPxxRPxU0pVkzxgubldu0cR1SUQcypPTDklPFA08T3Kj2S9xdhT9OTOrSG+n/FwE7I+IAwCSNpO11n6buCLi2w3X3w8s6VRoocQVEYfy98OS7s6DeUbScF7bGgYOF/1NzOaydaytOoT2unuquFDSgw3Ho3llZdJi4OmG4zHa16ZWA1/pdNOOiUvSi4F5EfHL/PNbgb8DtgKrgPX5+z2dyjKzBHSXuI40PLCbiqa5w/EXSm8mS1xv7HTTIjWuRcDdkiav/9eI+KqkXcAWSauBg8A1Bcoys7rr7XCIMWBpw/ES4FDrRZJeC9wOXBkRP+5UaMfElbdNj+vlzwu/vNPPm1kCzcNWvevj2gUsk3Qu8CPgWuC6xgskvQy4C3h3RPygSKEeOW9mzXo4VzEiJiTdBGwHhoCNEbFb0g359xuAjwAvBT6Vt+wmOjQ/nbjMrEWPR85HxDZgW8u5DQ2f3we8r5synbjMrJlXQDWzJA3CJGszm0MGZXUIM5tDvJCgmSXHNS4zS5ITl5klJYGFBJ24rO8m10OfS5IbKd/IwyHMLDnu4zKz5ByjZwsJ9osTl5kdz01FM0tOzdcqns1mGWZmlXDiMrPkOHGZWXLcx2VmLer/WNGJy8xa1H/ovBOXmbWo/whUJy6zWWic2jM4U5tc4zKz5DhxmVlyAnfO26y1NkFSW3lgMJtTx0vt32V67uMys+TUv6lYaACqpNMkfVHSE5L2SnqDpDMk7ZD0ZP5+er+DNbMyTNa4iryqUbTG9UngqxHxDkkvAk4GPgzsjIj1ktYAa4Bb+hTnnDY4TZDjf5d2Tceiv3djGb34Ww1yc7aYAahxSToVeBPwGYCIeCEifgasBDbll20Cru5PiGZWrsGocb0c+B/gs5LOAx4CbgYWRcQ4QESMSzprqh+WNAKMZEcLehCymfVX/af8FOnjOgG4EPh0RFwA/JqsWVhIRIxGxPKIWJ61MM2s3iabikVe1ShS4xoDxiLigfz4i2SJ6xlJw3ltaxg43K8gbXD1ok+qyj7AXvev1Ue9h0N0rHFFxH8DT0t6ZX7qcmAPsBVYlZ9bBdzTlwjNrGSDUeMCeD9wZ/5E8QBwPVnS2yJpNXAQuKY/IZpZuer/VLFQ4oqIR4HlU3x1eU+jMUvMYDUPJ3nkvJklp/5PFZ24zKzFgDQVzWwucVPRrFIzGa4wmP1W3ehtjUvSCrJpg0PA7RGxvuV75d9fBTwLvDciHm5XphOXmbXoXY1L0hBwG3AF2ZjQXZK2RsSehsuuBJblr4uBT+fv03LiMrMWPe2cvwjYHxEHACRtJpvn3Ji4VgKfi4gA7s9XoxmenFI4lZIT1/gRWPdfwELgSLn3npLjaDZwcbQuwVhVHLNQKIaG3+x3Z3/L8e3w0YUFLz5R0oMNx6MRMdpwvBh4uuF4jONrU1NdsxioR+KKiDMBJD2YzV2sluNwHHWPo4oYImJFD4vTVLeYwTVNvJO1mfXTGLC04XgJcGgG1zRx4jKzftoFLJN0bj5l8Fqyec6NtgLvUeb1wM/b9W9BdZ3zo50vKYXjaOY4mtUhjjrEMGMRMSHpJmA72XCIjRGxW9IN+fcbgG1kQyH2kw2HuL5Tuco68s3M0uGmopklx4nLzJJTauKStELSPkn7852ByrrvRkmHJT3ecK707dUkLZX0jXyLt92Sbq4iFkknSvqupO/lcayrIo6GeIYkPSLp3qrikPSUpO9LenRyXFJFcXgrwAJKS1wNQ/+vBF4NvFPSq0u6/R1A69iUNWTbqy0DdtLFOvqzMAF8MCL+AHg9cGP+Nyg7lueByyLiPOB8YEX+NKeKvwlkm6/sbTiuKo43R8T5DeOmqohjcivAVwHnkf1dqvp71FdElPIC3gBsbzi+Fbi1xPufAzzecLwPGM4/DwP7yoqlIYZ7yOZwVRYL2Q4mD5ONZi49DrIxOzuBy4B7q/q3AZ4CFracKzUO4FTgh+QPzaqKI4VXmU3F6Yb1V6VpezVgyu3V+kXSOcAFwANVxJI3zx4l2+RkR2SboVTxN/kE8CGyCXKTqogjgK9JeijfUq+KOBq3AnxE0u2SXlxBHLVXZuLqelj/oJL0EuBLwAci4hdVxBARRyPifLIaz0WSXlN2DJLeBhyOiIfKvvcULomIC8m6Mm6U9KYKYpjVVoBzSZmJq+th/X32TL6tGmVuryZpPlnSujMi7qoyFoDIdiW/j6wPsOw4LgHeLukpYDNwmaTPVxAHEXEofz8M3E22qkHZcUy1FeCFFcRRe2UmriJD/8tU+vZq+YJpnwH2RsTHq4pF0pmSTss/nwS8BXii7Dgi4taIWBIR55D99/D1iHhX2XFIerGkUyY/A28FHi87jvBWgMWV2aFGNqz/B8B/An9b4n2/QLZExm/I/q+2GngpWafwk/n7GSXE8Uay5vFjwKP566qyYwFeCzySx/E48JH8fOl/k4aYLuX/O+fL/nu8HPhe/to9+d9mRf+NnA88mP/bfBk4vcp/l7q+POXHzJLjkfNmlhwnLjNLjhOXmSXHicvMkuPEZWbJceIys+Q4cZlZcv4P0ZpiqL0fRBoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_image(1, predictions[5], \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "above-communication",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 20):\n",
    "    image = predictions[i]\n",
    "    image[image!=0] = 1\n",
    "    predictions[i] = image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "dress-partnership",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0170166\n"
     ]
    }
   ],
   "source": [
    "mse = keras.losses.MeanSquaredError()\n",
    "\n",
    "c = mse(true_img, np.reshape(predictions, (20, 64, 64, 1))).numpy()\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "precise-flash",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
